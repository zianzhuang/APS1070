{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTpnyUEJrAjE"
      },
      "source": [
        "# APS1070, Tutorial 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn-UWaxnwA4K"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS-L6yG_wA4L"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mFMzlbcbcs0"
      },
      "source": [
        "In statistics, linear regression is a linear approach to modelling the relationship between a dependent variable and one or more independent variables. Let $x$ be the independent variable and $y$ be the dependent variable. We will define a linear relationship between these two variables as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3JNdY_Cbcs0"
      },
      "source": [
        "$y = wX + b$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YAZTV1Mbcs1"
      },
      "source": [
        "This is the equation for a line that you studied in high school. $w$ is the slope of the line and b is the $y$ intercept. Today we will use this equation to train our model with a given dataset and predict the value of $y$ for any given value of $x$. Our challenge today is to determine the value of $w$ and $b$, such that the line corresponding to those values is the best fitting line or gives the minimum error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ENvOgIIbcs1"
      },
      "source": [
        "Are we always fitting a line into the data? What if we have more than 1 feature (independant variable)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhIIg3Pbcs2"
      },
      "source": [
        "The vectorized form of above equation is written as $y = Xw$, where $y$ and $w$ are vectors while $X$ is a matrix. \n",
        "\n",
        "Where is the b term? It is included within the $X$ matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyYxxkfAbcs2"
      },
      "source": [
        "**Hypothesis of Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUz5wFvUbcs3"
      },
      "source": [
        "The linear regression model can be represented by the following equation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyqwDStzWaAB"
      },
      "source": [
        "\n",
        "\n",
        "$y= b + w_1x_1 + w_2x_2 + ......+w_nx_n$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPdf8fFhaU50"
      },
      "source": [
        "The bias term can further be incorporated into the quation as an additional weight with cofficient 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADMnqhJDaeiF"
      },
      "source": [
        "$y= w_0(1) + w_1x_1 + w_2x_2 + ......+w_nx_n$, where $w_0 = b$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_aOnL22bcs4"
      },
      "source": [
        "- $y$ is the predicted value ($h_w(x)$)\n",
        "- $w_0$ is the bias term.\n",
        "- $w_i$, where i>0 are the model parameters\n",
        "- $x_i$, where i>0 are the feature values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD4i39QVwA4O"
      },
      "source": [
        "### Loss and Cost functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6AiJnqnbcs5"
      },
      "source": [
        "Our Loss function for Linear regression would be sum of squares, which makes the cost function to be:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUIa2Og3bcs5"
      },
      "source": [
        "\n",
        "$L(y,t)=\\frac{1}{2}\\ (y-t)^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53bIKpy5bcs6"
      },
      "source": [
        "Here $t$ is the actual value and $y$ is the predicted value. \n",
        "For the simplest case lets assume a straight line with folowing equation Lets substitute the value of $y$ from $y = wx + b$ and compute the average for $N$ examples ):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZERyj_zcpku"
      },
      "source": [
        " Cost Function: $J=\\frac{1}{2N}[\\sum_{i=1}^N((wx^{(i)}+b)-t^{(i)})^2]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5GC9wklbcs7"
      },
      "source": [
        "Partial Derivative of the above equation with respect to $w$ is shown here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKB_CwH_dwfU"
      },
      "source": [
        "$\\frac{{\\partial J}}{\\partial w}=\\frac{1}{2N}[\\sum_{i=0}^N2~x^{(i)}~((wx^{(i)}+b)-t^{(i)})]$\n",
        "\n",
        "$\\frac{{\\partial J}}{\\partial w}=\\frac{1}{N}[\\sum_{i=0}^Nx^{(i)}(y^{(i)}-t^{(i)})]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4bzLgyXbcs7"
      },
      "source": [
        "While the Partial Derivative with respect to $b$ is shown here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wLmX5mAgslo"
      },
      "source": [
        "$\\frac{\\partial J}{\\partial b}=\\frac{1}{N}[\\sum_{i=0}^N(y^{(i)}-t^{(i)})]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNAgpf18wA4S"
      },
      "source": [
        "#### Direct Solution\n",
        "\n",
        "Can you derive the Analytical Solution for Linear Regression?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-vd5YWLbcs-"
      },
      "source": [
        "We arrive at the analytical solution when we turn the partial derivatives with respect to the parameters to zero. $\\frac{{\\partial J}}{\\partial w}=0$ and  $\\frac{{\\partial J}}{\\partial b}=0$.\n",
        "\n",
        "This is because at the point where cost function is at the minimum with respect to the parameters ($w$ and $b$), the derivative of cost function with respect to the parameters would be zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_qnAm8tbcs_"
      },
      "source": [
        "The solution for the general case comes out to be:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91CsuqsUbcs_"
      },
      "source": [
        "$w = (X^TX)^{-1}X^Tt$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuHJAOmvbctA"
      },
      "source": [
        "With $L_2$ Regularization. Cost:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCDCGqdbbctB"
      },
      "source": [
        "$J(w) = \\frac{1}{2N}[\\sum_{i=1}^N(h_w (x^{(i)}) - t^{(i)})^2 + \\lambda\\sum_{j=1}^nw^2_j]$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w484FOiDbctC"
      },
      "source": [
        "$w = (X^TX + \\lambda I)^{-1}X^Tt$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPNKfSh1bctG"
      },
      "source": [
        "### Practice Direct solution\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pUP_nOYbctG"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aqKk5Q0rcMp"
      },
      "source": [
        "We will start with fitting distribution of data points with a line followed by higher order polynomials to visualize the idea behind it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2s-ZTXtbctI"
      },
      "source": [
        "n = 50 # number of points\n",
        "w = 2 # slope of line\n",
        "b = 4 # y intercept of line\n",
        "r = 2 # range of data on x-axis\n",
        "\n",
        "np.random.seed(2)\n",
        "x =  r*np.random.rand(n)\n",
        "x.sort()\n",
        "yPerf = w*x + b # perfect world: no noise\n",
        "y = w*x + b + 0.2 * np.random.randn(n) # imperfect world: we get noisy data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlW5IUn5bctJ"
      },
      "source": [
        "Spoiler Alert: We are in an Imperfect World."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNe4w9CdbctJ"
      },
      "source": [
        "def rmse(yPred, y):\n",
        "    return np.sqrt(mean_squared_error(yPred, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmkB-_lpbctL"
      },
      "source": [
        "plt.plot(x, y, 'o', label='Data')\n",
        "plt.plot(x, yPerf, 'o-', label='Underlying Distribution')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL7-QF9TbctN"
      },
      "source": [
        "x = np.vstack((np.ones(np.shape(x)), x)).T\n",
        "y = y.reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70gnrF0XbctO"
      },
      "source": [
        "# analytical solution\n",
        "W = np.dot(np.linalg.inv(np.dot(x.T, x)), np.dot(x.T, y))\n",
        "\n",
        "# prediction\n",
        "yPred = np.dot(x, W)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJqxsROhPEGw"
      },
      "source": [
        "W[0],W[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlvll92zbctS"
      },
      "source": [
        "- How similar are these to the values we had set initially?\n",
        "- Will they be same as those set initially if there was no noise?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyxfDFgfbctS"
      },
      "source": [
        "plt.plot(x[:, 1], y, 'o', label='Data')\n",
        "plt.plot(x[:, 1], yPerf, 'o-', label='Underlying Distribution')\n",
        "plt.plot(x[:, 1], yPred, 'o-', label='Predicted')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l5yQVtFbctU"
      },
      "source": [
        "print('RMSE Perf: ', rmse(yPerf, y))\n",
        "print('RMSE Pred: ', rmse(yPred, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ZG5e0YbctW"
      },
      "source": [
        "### Feature Mapping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9eFKueSbctX"
      },
      "source": [
        "n = 200 # number of points\n",
        "r = 6  # range of data on x-axis\n",
        "\n",
        "np.random.seed(10)\n",
        "X = xD = r * np.random.random(n) # points also stored in xD (xData). will be useful later.\n",
        "X.sort()\n",
        "yPerf = X - 3 * (X ** 2) + 0.5 * (X ** 3) \n",
        "np.random.seed(10)\n",
        "y = yPerf +  0.2 * np.random.normal(0, 5, n)  # imperfect world: we get noisy data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFpKR94YbctY"
      },
      "source": [
        "plt.plot(X, y, 'o', label='Data')\n",
        "plt.plot(X, yPerf, 'o-', label='Underlying Distribution')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAwZSKq7nU9r"
      },
      "source": [
        "X = np.vstack((np.ones(np.shape(X)), X)).T\n",
        "y = y.reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZy6EK4Mbctb"
      },
      "source": [
        "# analytical solution\n",
        "W = np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))\n",
        "\n",
        "# prediction\n",
        "yPredLinear = yPred = np.dot(X, W)\n",
        "\n",
        "W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS_2S74qbctc"
      },
      "source": [
        "plt.plot(xD, y, 'o', label='Data')\n",
        "plt.plot(xD, yPerf, 'o-', label='Underlying Distribution')\n",
        "plt.plot(xD, yPred, 'o-', label='Predicted')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkKlZkKkbctd"
      },
      "source": [
        "print('RMSE: ', rmse(yPred, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6zNdggSbctf"
      },
      "source": [
        " What to do next?\n",
        "\n",
        "Can we add more features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIDKcjBKbctf"
      },
      "source": [
        "X = np.vstack((X.T, xD**2, xD**3)).T\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlhuePv2bcth"
      },
      "source": [
        "# analytical solution\n",
        "W = np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))\n",
        "\n",
        "# prediction\n",
        "yPred4Feature = yPred = np.dot(X, W)\n",
        "W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As3fF2SMbctj"
      },
      "source": [
        "plt.plot(xD, y, 'o', label='Data')\n",
        "plt.plot(xD, yPerf, 'o-', label='Underlying Distribution')\n",
        "plt.plot(xD, yPred, 'o-', label='Predicted')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4w21j3Wbctl"
      },
      "source": [
        "print('RMSE: ', rmse(yPred, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UVOla-Obctm"
      },
      "source": [
        "But how do we know when to stop, since we would not be knowing when to stop adding features in x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1dphgtbbctm"
      },
      "source": [
        "X = np.vstack((X.T, xD**4, xD**5, xD**6, xD**7, xD**8, xD**9, xD**10, xD**11)).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9_6iGtVbcto"
      },
      "source": [
        "# analytical solution\n",
        "W = np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))\n",
        "\n",
        "# prediction\n",
        "yPred12Feature = yPred = np.dot(X, W)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz8q4G9Vbctp"
      },
      "source": [
        "W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wVphfLKbctq"
      },
      "source": [
        "plt.plot(xD, y, 'o', label='Data')\n",
        "plt.plot(xD, yPerf, 'o-', label='Underlying Distribution')\n",
        "plt.plot(xD, yPred, 'o-', label='Predicted')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB5FgxqCbcts"
      },
      "source": [
        "print('RMSE: ', rmse(yPred, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjUKcac3bctt"
      },
      "source": [
        "Is the model overfitted?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbgmXOxdbctu"
      },
      "source": [
        "When does the model overfits: when we have more features or comparitively less data for the model.\n",
        "\n",
        "What exactly is overfitting:\n",
        "It pays more attention to the noise of the data provided, in a sense trying to rotely memorize everything, \n",
        "without generalizing. \n",
        "\n",
        "Since we don't know when to stop adding features, what can be done:\n",
        "- Solution is to work with a model or feature set that can slightly overfit your data, and then use techniques to prevent overfitting from happening.\n",
        "The alternative gives us underfitting which we cannot fix unless you modify the feature set or model.\n",
        "\n",
        "Options we have to prevent overfitting. Well there are many, most widely used ones are\n",
        "- Using a validation set\n",
        "- Regularization: add penalty on weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZxmLfzjbctu"
      },
      "source": [
        "λ = 0.1 # what is lambda: regularization parameter\n",
        "f = 12 # number of features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWvLnUOTbctw"
      },
      "source": [
        "# analytical solution\n",
        "W_reg = np.dot(np.linalg.inv(np.dot(X.T, X) + (λ)*np.identity(f)), np.dot(X.T, y))\n",
        "\n",
        "# prediction\n",
        "yPred12FeatRegu = yPred = np.dot(X, W_reg)\n",
        "W\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQiyDIi_bctx"
      },
      "source": [
        "plt.plot(xD, y, 'o', label='Data')\n",
        "plt.plot(xD, yPerf, 'o-', label='Underlying Distribution')\n",
        "plt.plot(xD, yPred, 'o-', label='Predicted')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw4mgsnbbct0"
      },
      "source": [
        "Can we know from the plot if the value of λ is optimal:\n",
        "\n",
        "Somewhat but not exactly. \n",
        "\n",
        "To get the exact value of lambda you need to split dataset between training and testing. Then cycle over multiple values of lambda. The most optimum is the one which gives the lowest test error. \n",
        "\n",
        "What does low test error represent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcSDizyQbct0"
      },
      "source": [
        "All models together:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCYdZCQRbct1"
      },
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(xD, y, 'o', label='Data')\n",
        "plt.plot(xD, yPerf, 'o-', label='Underlying Distribution')\n",
        "plt.plot(xD, yPredLinear, 'o-', label='2 Feature')\n",
        "plt.plot(xD, yPred4Feature, 'o-', label='4 Feature')\n",
        "plt.plot(xD, yPred12Feature, 'o-', label='12 Feature')\n",
        "plt.plot(xD, yPred12FeatRegu, 'o-', label='12 Feature Regularized')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print('RMSE Y: ', rmse(y, y))\n",
        "print('RMSE Underlying Distribution: ', rmse(yPerf, y))\n",
        "print('RMSE 2 Feature: ', rmse(yPredLinear, y))\n",
        "print('RMSE 4 Feature: ', rmse(yPred4Feature, y))\n",
        "print('RMSE 12 Feature: ', rmse(yPred12Feature, y))\n",
        "print('RMSE 12 Feature Regularized: ', rmse(yPred12FeatRegu, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "iQyQ3hA6bctC"
      },
      "source": [
        "#### Gradient Descent Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "4nPHi1eSbctC"
      },
      "source": [
        "Gradient descent uses the equations for gradient derived above to find the direction in which we tinker the values of our parameters $w$ and $b$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcGpaxKYh8tk"
      },
      "source": [
        "\n",
        "$\\frac{{\\partial J}}{\\partial w}=\\frac{1}{N}[\\sum_{i=0}^Nx^{(i)}(y^{(i)}-t^{(i)})]$\n",
        "\n",
        "$w_j=w_j-\\alpha \\times \\frac{{\\partial J}}{\\partial w}$\n",
        "\n",
        "$b_j=b_j-\\alpha \\times \\frac{{\\partial J}}{\\partial b}$ {we embbed this in w!}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "TmX3-0DYbctD"
      },
      "source": [
        "Here the term $\\alpha$ is defined as the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGi4Evsss4QL"
      },
      "source": [
        "![link text](https://www.eecg.utoronto.ca/~hadizade/APS1070/1.PNG)\n",
        "![link text](https://www.eecg.utoronto.ca/~hadizade/APS1070/2.PNG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a0mIQ1PZWoQ"
      },
      "source": [
        "import math\n",
        "n = 500 # number of points\n",
        "w = 7 # slope of line\n",
        "b = -10 # y intercept of line\n",
        "r = 2 # range of data on x-axis\n",
        "w_i=[-10,7,10,20]\n",
        "np.random.seed(2)\n",
        "x =  r*np.random.rand(n)\n",
        "t = (w_i[1]*x  +  w_i[2]* np.sin(math.pi*x*2) + w_i[3]*np.cos(math.pi*x*3) + w_i[0] + 0.2 * np.random.randn(n)).reshape(-1,1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HTjGERnmQR4"
      },
      "source": [
        "plt.scatter(x,t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on-xUcSwZ7zR"
      },
      "source": [
        "X=x.reshape(-1,1)\n",
        "X= np.hstack((np.ones(np.shape(X)), X ,np.sin(math.pi*X*2), np.cos(math.pi*X*3) ))\n",
        "X[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RodinmiIZaQx"
      },
      "source": [
        "w = (np.random.random(4)).reshape(1,-1) ### Inital weights\n",
        "lr = 0.01  ### Learning rate\n",
        "rmse_array=[]\n",
        "\n",
        "for epoch in range (0, 3000): \n",
        "  y= np.dot(X,w.T).reshape (-1,1)\n",
        "  rmse_array.append(rmse(y,t))\n",
        "  gradient = (1/len(y) * np.dot(X.T, y-t)).reshape(1,-1)\n",
        "  w = w - lr * gradient ### weight update\n",
        "\n",
        "\n",
        "print (\"X: \", X.shape)\n",
        "print (\"w: \", w.shape)\n",
        "print (\"y: \", y.shape)\n",
        "print (\"t: \", t.shape)\n",
        "print (\"gradient: \", gradient.shape)\n",
        "plt.plot(rmse_array)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU-Zyy6OewyK"
      },
      "source": [
        "w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIbGjDjxteGh"
      },
      "source": [
        "#Mini-batch Gradient Descent!\n",
        "![link text](https://www.eecg.utoronto.ca/~hadizade/APS1070/3.PNG)"
      ]
    }
  ]
}